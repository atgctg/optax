{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXQz7Vp8ehqb"
      },
      "source": [
        "# Learn Optax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKcocLxEyYf2"
      },
      "source": [
        "## Quick Start\n",
        "\n",
        "Let's use optax to fit a parametrized function. We will consider the problem of learning to identify when a value is odd or even.\n",
        "\n",
        "We will begin by creating a dataset that consists of batches of random 8 bit integers (represented using their binary representation), with each value labelled as \"odd\" or \"even\" using 1-hot encoding (i.e. `[1, 0]` means odd `[0, 1]` means even).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Gg6zyMBqydty"
      },
      "outputs": [],
      "source": [
        "import optax\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "\n",
        "BATCH_SIZE = 5\n",
        "NUM_TRAIN_STEPS = 1_000\n",
        "RAW_TRAINING_DATA = jax.random.randint(jax.random.PRNGKey(42), (NUM_TRAIN_STEPS, BATCH_SIZE, 1), 0, 255)\n",
        "\n",
        "TRAINING_DATA = jnp.unpackbits(RAW_TRAINING_DATA.astype(jnp.uint8), axis=-1)\n",
        "LABELS = jax.nn.one_hot(RAW_TRAINING_DATA % 2, 2).astype(jnp.float32).reshape(NUM_TRAIN_STEPS, BATCH_SIZE, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV79rjQK8tvC"
      },
      "source": [
        "We may now define a parametrized function using JAX. This will allow us to efficiently compute gradients.\n",
        "\n",
        "There are a number of libraries that provide common building blocks for parametrized functions (such as flax and haiku). For this case though, we shall implement our function from scratch.\n",
        "\n",
        "Our function will be a 1-layer MLP (multi-layer perceptron) with a single hidden layer, and a single output layer. We initialize all parameters using a standard Gaussian $\\mathcal{N}(0,1)$ distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Syp9LJ338h9-"
      },
      "outputs": [],
      "source": [
        "initial_params = {\n",
        "    'hidden': jax.random.normal(shape=[8, 32], key=jax.random.PRNGKey(0)),\n",
        "    'output': jax.random.normal(shape=[32, 2], key=jax.random.PRNGKey(1)),\n",
        "}\n",
        "\n",
        "\n",
        "def net(x: jnp.ndarray, params: jnp.ndarray) -> jnp.ndarray:\n",
        "  x = jnp.dot(x, params['hidden'])\n",
        "  x = jax.nn.relu(x)\n",
        "  x = jnp.dot(x, params['output'])\n",
        "  return x\n",
        "\n",
        "\n",
        "def loss(params: optax.Params, batch: jnp.ndarray, labels: jnp.ndarray) -> jnp.ndarray:\n",
        "  y_hat = net(batch, params)\n",
        "\n",
        "  # optax also provides a number of common loss functions.\n",
        "  loss_value = optax.sigmoid_binary_cross_entropy(y_hat, labels).sum(axis=-1)\n",
        "\n",
        "  return loss_value.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LVHrJyH9vDe"
      },
      "source": [
        "We will use `optax.adam` to compute the parameter updates from their gradients on each optimizer step.\n",
        "\n",
        "Note that since optax optimizers are implemented using pure functions, we will need to also keep track of the optimizer state. For the Adam optimizer, this state will contain the momentum values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "executionInfo": {
          "elapsed": 6046,
          "status": "ok",
          "timestamp": 1636155226542,
          "user": {
            "displayName": "Ross Hemsley",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSZqBnQizDvVofyb2N_r9W3cP4duk9mv1mxCb9=s64",
            "userId": "11415908946302743815"
          },
          "user_tz": 0
        },
        "id": "JsbPBTF09FGY",
        "outputId": "c427f94f-a605-44fc-b519-707bc5d47b7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step:   0, Loss: 5.624\n",
            "Step: 100, Loss: 0.188\n",
            "Step: 200, Loss: 0.053\n",
            "Step: 300, Loss: 0.025\n",
            "Step: 400, Loss: 0.004\n",
            "Step: 500, Loss: 0.028\n",
            "Step: 600, Loss: 0.002\n",
            "Step: 700, Loss: 0.025\n",
            "Step: 800, Loss: 0.017\n",
            "Step: 900, Loss: 0.003\n"
          ]
        }
      ],
      "source": [
        "@jax.jit\n",
        "def step(params, opt_state, batch, labels):\n",
        "  loss_value, grads = jax.value_and_grad(loss)(params, batch, labels)\n",
        "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, opt_state, loss_value\n",
        "\n",
        "def fit(params: optax.Params, optimizer: optax.GradientTransformation) -> optax.Params:\n",
        "  opt_state = optimizer.init(params)\n",
        "\n",
        "  for i, (batch, labels) in enumerate(zip(TRAINING_DATA, LABELS)):\n",
        "    params, opt_state, loss_value = step(params, opt_state, batch, labels)\n",
        "    if i % 100 == 0:\n",
        "      print(f'Step: {i:3}, Loss: {loss_value:.3f}')\n",
        "\n",
        "  return params\n",
        "\n",
        "# Finally, we can fit our parametrized function using the Adam optimizer\n",
        "# provided by optax.\n",
        "optimizer = optax.adam(learning_rate=1e-2)\n",
        "params = fit(initial_params, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTaBLYL8_Ppz"
      },
      "source": [
        "We see that our loss appears to have converged, which should indicate that we have successfully found better parameters for our network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT_Uaei5Dv_3"
      },
      "source": [
        "## Weight Decay, Schedules and Clipping\n",
        "\n",
        "Many research models make use of techniques such as learning rate scheduling, and gradient clipping. These may be achieved by _chaining_ together gradient transformations such as `optax.adam` and `optax.clip`.\n",
        "\n",
        "In the following, we will use `Adam` with weight decay (`optax.adamw`), a cosine learning rate schedule (with warmup) and also gradient clipping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "executionInfo": {
          "elapsed": 734,
          "status": "ok",
          "timestamp": 1636155227388,
          "user": {
            "displayName": "Ross Hemsley",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSZqBnQizDvVofyb2N_r9W3cP4duk9mv1mxCb9=s64",
            "userId": "11415908946302743815"
          },
          "user_tz": 0
        },
        "id": "SZegYQajDtLi",
        "outputId": "f65f9fd8-8e9c-4ae6-e759-62362ff94f53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step:   0, Loss: 5.624\n",
            "Step: 100, Loss: 0.000\n",
            "Step: 200, Loss: 0.000\n",
            "Step: 300, Loss: 0.000\n",
            "Step: 400, Loss: 0.000\n",
            "Step: 500, Loss: 0.000\n",
            "Step: 600, Loss: 0.000\n",
            "Step: 700, Loss: 0.000\n",
            "Step: 800, Loss: 0.000\n",
            "Step: 900, Loss: 0.000\n"
          ]
        }
      ],
      "source": [
        "schedule = optax.warmup_cosine_decay_schedule(\n",
        "  init_value=0.0,\n",
        "  peak_value=1.0,\n",
        "  warmup_steps=50,\n",
        "  decay_steps=1_000,\n",
        "  end_value=0.0,\n",
        ")\n",
        "\n",
        "optimizer = optax.chain(\n",
        "  optax.clip(1.0),\n",
        "  optax.adamw(learning_rate=schedule),\n",
        ")\n",
        "\n",
        "params = fit(initial_params, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7-efvtM16pO"
      },
      "source": [
        "## Reading the Learning Rate inside the Train Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzPJMRYV16pP"
      },
      "source": [
        "Sometimes we want to access certain hyperparameters in the optimizer. For example, we may want to log the learning rate at a service.\n",
        "\n",
        "To extract the learning rate inside the train loop, we can use the [inject_hyperparams](https://optax.readthedocs.io/en/latest/api.html#optax.inject_hyperparams) wrapper to make any hyperparameter a modifiable part of the optimizer state. This means that you can promote the learning rate to be part of the optimizer state so that you can access it in the optimizer state directly.\n",
        "\n",
        "The following example demonstrates how to extend the previous code to extract the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIT1aO9_16pP",
        "outputId": "f90205ee-9359-42b3-f745-15aa67d33b62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available hyperparams: b1 b2 eps eps_root weight_decay learning_rate\n",
            "\n",
            "Step   0, Loss: 5.624, Learning rate: 0.020\n",
            "Step 100, Loss: 0.000, Learning rate: 0.993\n",
            "Step 200, Loss: 0.000, Learning rate: 0.939\n",
            "Step 300, Loss: 0.000, Learning rate: 0.837\n",
            "Step 400, Loss: 0.000, Learning rate: 0.699\n",
            "Step 500, Loss: 0.000, Learning rate: 0.540\n",
            "Step 600, Loss: 0.000, Learning rate: 0.376\n",
            "Step 700, Loss: 0.000, Learning rate: 0.225\n",
            "Step 800, Loss: 0.000, Learning rate: 0.104\n",
            "Step 900, Loss: 0.000, Learning rate: 0.027\n"
          ]
        }
      ],
      "source": [
        "# Wrap the optimizer to inject the hyperparameters\n",
        "optimizer = optax.inject_hyperparams(optax.adamw)(learning_rate=schedule)\n",
        "\n",
        "def fit(params: optax.Params, optimizer: optax.GradientTransformation) -> optax.Params:\n",
        "  opt_state = optimizer.init(params)\n",
        "\n",
        "  # Since we injected hyperparams, we can access them directly here\n",
        "  print(f'Available hyperparams: {\" \".join(opt_state.hyperparams.keys())}\\n')\n",
        "\n",
        "  for i, (batch, labels) in enumerate(zip(TRAINING_DATA, LABELS)):\n",
        "    params, opt_state, loss_value = step(params, opt_state, batch, labels)\n",
        "    if i % 100 == 0:\n",
        "      # Get the updated learning rate\n",
        "      lr = opt_state.hyperparams['learning_rate']\n",
        "      print(f'Step {i:3}, Loss: {loss_value:.3f}, Learning rate: {lr:.3f}')\n",
        "\n",
        "  return params\n",
        "\n",
        "params = fit(initial_params, optimizer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "optax-101.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "626d743d6476408aa1b36c3ff0d1f9d9d03e37c6879626ddfcdd13d658004bbf"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
